{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.101:4040\n",
       "SparkContext available as 'sc' (version = 2.4.0, master = local[*], app id = local-1551029661403)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
       "import org.apache.spark.mllib.tree.RandomForest\n",
       "import org.apache.spark.mllib.tree.model.RandomForestModel\n",
       "import org.apache.spark.ml.classification.RandomForestClassifier\n",
       "import org.apache.spark.ml.feature.VectorIndexer\n",
       "import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\n",
       "import org.apache.spark.mllib.linalg.Vectors\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "\n",
    "import org.apache.spark.mllib.tree.RandomForest\n",
    "import org.apache.spark.mllib.tree.model.RandomForestModel\n",
    "\n",
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "\n",
    "import org.apache.spark.ml.feature.VectorIndexer\n",
    "import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\n",
    "\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-24 18:34:27 WARN  SparkSession$Builder:66 - Using an existing SparkSession; some configuration may not take effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "session: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@517403b2\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val session: SparkSession = SparkSession\n",
    "    .builder()\n",
    "    .master(\"local[4]\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataSetPath: String = /home/alcrd/projects/modern scala projects/chapter_01/iris.csv\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataSetPath = \"/home/alcrd/projects/modern scala projects/chapter_01/iris.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buildDataFrame: (dataSet: String)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def buildDataFrame(dataSet: String): DataFrame = {\n",
    "    session.read.format(\"com.databricks.spark.csv\")\n",
    "      .option(\"header\", true)\n",
    "      .option(\"inferSchema\", true)\n",
    "      .load(dataSet)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Id: int, SepalLengthCm: double ... 4 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = buildDataFrame(dataSetPath)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|summary|                Id|     SepalLengthCm|       SepalWidthCm|     PetalLengthCm|      PetalWidthCm|       Species|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|  count|               150|               150|                150|               150|               150|           150|\n",
      "|   mean|              75.5| 5.843333333333335| 3.0540000000000007|3.7586666666666693|1.1986666666666672|          null|\n",
      "| stddev|43.445367992456916|0.8280661279778637|0.43359431136217375| 1.764420419952262|0.7631607417008414|          null|\n",
      "|    min|                 1|               4.3|                2.0|               1.0|               0.1|   Iris-setosa|\n",
      "|    max|               150|               7.9|                4.4|               6.9|               2.5|Iris-virginica|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "summary: org.apache.spark.sql.DataFrame = [summary: string, Id: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val summary = df.describe(\"Id\",\"SepalLengthCm\",\"SepalWidthCm\",\"PetalLengthCm\",\"PetalWidthCm\",\"Species\")\n",
    "summary.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_a76f54aff412\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"))\n",
    "  .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+-----------------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|         features|\n",
      "+---+-------------+------------+-------------+------------+-----------+-----------------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|[4.7,3.2,1.3,0.2]|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|[4.6,3.1,1.5,0.2]|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|[5.0,3.6,1.4,0.2]|\n",
      "+---+-------------+------------+-------------+------------+-----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_transformed: org.apache.spark.sql.DataFrame = [Id: int, SepalLengthCm: double ... 5 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_transformed = assembler.transform(df)\n",
    "df_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-----------+-----------------+-----+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|         features|Label|\n",
      "+---+-------------+------------+-------------+------------+-----------+-----------------+-----+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|  0.0|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|  0.0|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|[4.7,3.2,1.3,0.2]|  0.0|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|[4.6,3.1,1.5,0.2]|  0.0|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|[5.0,3.6,1.4,0.2]|  0.0|\n",
      "+---+-------------+------------+-------------+------------+-----------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "indexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_1c578cbd7012\n",
       "df_in: org.apache.spark.sql.DataFrame = [Id: int, SepalLengthCm: double ... 6 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indexer = new StringIndexer()\n",
    "  .setInputCol(\"Species\")\n",
    "  .setOutputCol(\"Label\")\n",
    "  .fit(df_transformed)\n",
    "  \n",
    "val df_in = indexer.transform(df_transformed)\n",
    "df_in.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|label|         features|\n",
      "+-----+-----------------+\n",
      "|  0.0|[5.1,3.5,1.4,0.2]|\n",
      "|  0.0|[4.9,3.0,1.4,0.2]|\n",
      "|  0.0|[4.7,3.2,1.3,0.2]|\n",
      "|  0.0|[4.6,3.1,1.5,0.2]|\n",
      "|  0.0|[5.0,3.6,1.4,0.2]|\n",
      "|  0.0|[5.4,3.9,1.7,0.4]|\n",
      "|  0.0|[4.6,3.4,1.4,0.3]|\n",
      "|  0.0|[5.0,3.4,1.5,0.2]|\n",
      "|  0.0|[4.4,2.9,1.4,0.2]|\n",
      "|  0.0|[4.9,3.1,1.5,0.1]|\n",
      "|  0.0|[5.4,3.7,1.5,0.2]|\n",
      "|  0.0|[4.8,3.4,1.6,0.2]|\n",
      "|  0.0|[4.8,3.0,1.4,0.1]|\n",
      "|  0.0|[4.3,3.0,1.1,0.1]|\n",
      "|  0.0|[5.8,4.0,1.2,0.2]|\n",
      "|  0.0|[5.7,4.4,1.5,0.4]|\n",
      "|  0.0|[5.4,3.9,1.3,0.4]|\n",
      "|  0.0|[5.1,3.5,1.4,0.3]|\n",
      "|  0.0|[5.7,3.8,1.7,0.3]|\n",
      "|  0.0|[5.1,3.8,1.5,0.3]|\n",
      "+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_subset: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_subset = df_in.select(\"label\", \"features\")\n",
    "df_subset.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfs: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([label: double, features: vector], [label: double, features: vector])\n",
       "df_train: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n",
       "df_test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfs = df_subset.randomSplit(Array(.80, .20))\n",
    "val (df_train, df_test) = (dfs(0), dfs(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_a1ac7aad113a\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rf = new RandomForestClassifier()\n",
    "    .setLabelCol(\"label\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setNumTrees(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_8e42774e7516\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gridSearch: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\trfc_a1ac7aad113a-impurity: gini,\n",
       "\trfc_a1ac7aad113a-maxDepth: 4\n",
       "}, {\n",
       "\trfc_a1ac7aad113a-impurity: entropy,\n",
       "\trfc_a1ac7aad113a-maxDepth: 4\n",
       "}, {\n",
       "\trfc_a1ac7aad113a-impurity: gini,\n",
       "\trfc_a1ac7aad113a-maxDepth: 5\n",
       "}, {\n",
       "\trfc_a1ac7aad113a-impurity: entropy,\n",
       "\trfc_a1ac7aad113a-maxDepth: 5\n",
       "}, {\n",
       "\trfc_a1ac7aad113a-impurity: gini,\n",
       "\trfc_a1ac7aad113a-maxDepth: 6\n",
       "}, {\n",
       "\trfc_a1ac7aad113a-impurity: entropy,\n",
       "\trfc_a1ac7aad113a-maxDepth: 6\n",
       "}, {\n",
       "\trfc_a1ac7aad113a-impurity: gini,\n",
       "\trfc_a1ac7aad113a-maxDepth: 10\n",
       "}, {\n",
       "\trfc_a1ac7aad113a-impurity: entropy,\n",
       "\trfc_a1ac7aad113a-maxDepth: 10\n",
       "})\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gridSearch = new ParamGridBuilder()\n",
    "    .addGrid(rf.maxDepth, Array(4, 5, 6, 10))\n",
    "    .addGrid(rf.impurity, Array(\"gini\", \"entropy\"))\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainValidationSplit: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_95c8d79b942e\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "  .setEstimator(rf)\n",
    "  .setEvaluator(evaluator)\n",
    "  .setEstimatorParamMaps(gridSearch)\n",
    "  .setTrainRatio(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.tuning.TrainValidationSplitModel = tvs_95c8d79b942e\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = trainValidationSplit.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pred: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 3 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pred = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+--------------------+--------------------+----------+\n",
      "|label|         features|       rawPrediction|         probability|prediction|\n",
      "+-----+-----------------+--------------------+--------------------+----------+\n",
      "|  0.0|[4.4,2.9,1.4,0.2]|      [10.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|  0.0|[4.6,3.4,1.4,0.3]|      [10.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|  0.0|[4.8,3.1,1.6,0.2]|      [10.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|  0.0|[4.8,3.4,1.6,0.2]|      [10.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|  0.0|[4.8,3.4,1.9,0.2]|[8.0,1.9555555555...|[0.8,0.1955555555...|       0.0|\n",
      "|  0.0|[4.9,3.0,1.4,0.2]|      [10.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|  0.0|[5.0,3.2,1.2,0.2]|      [10.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|  0.0|[5.1,3.3,1.7,0.5]|[9.0,0.9555555555...|[0.9,0.0955555555...|       0.0|\n",
      "|  0.0|[5.1,3.4,1.5,0.2]|      [10.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|  0.0|[5.1,3.5,1.4,0.3]|      [10.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|  0.0|[5.5,4.2,1.4,0.2]|      [10.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|  0.0|[5.7,4.4,1.5,0.4]|[9.0,0.9354838709...|[0.9,0.0935483870...|       0.0|\n",
      "|  1.0|[5.4,3.0,4.5,1.5]|[0.0,9.9285285285...|[0.0,0.9928528528...|       1.0|\n",
      "|  1.0|[5.5,2.4,3.8,1.1]|[0.0,9.9285285285...|[0.0,0.9928528528...|       1.0|\n",
      "|  1.0|[5.7,2.6,3.5,1.0]|[0.0,9.8640123994...|[0.0,0.9864012399...|       1.0|\n",
      "|  1.0|[6.0,2.7,5.1,1.6]|[0.0,2.1222222222...|[0.0,0.2122222222...|       2.0|\n",
      "|  1.0|[6.6,3.0,4.4,1.4]|[0.0,9.7799283154...|[0.0,0.9779928315...|       1.0|\n",
      "|  1.0|[6.7,3.1,4.7,1.5]|[0.0,9.7799283154...|[0.0,0.9779928315...|       1.0|\n",
      "|  1.0|[6.9,3.1,4.9,1.5]|[0.0,5.8444444444...|[0.0,0.5844444444...|       1.0|\n",
      "|  2.0|[7.7,2.8,6.7,2.0]|      [0.0,0.0,10.0]|       [0.0,0.0,1.0]|       2.0|\n",
      "+-----+-----------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.04761904761904767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "accuracy: Double = 0.9523809523809523\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accuracy = evaluator.evaluate(pred)\n",
    "println(s\"Test Error = ${(1.0 - accuracy)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
